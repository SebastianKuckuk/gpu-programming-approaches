{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af799ed6",
   "metadata": {},
   "source": [
    "# Reductions on GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df23d0db",
   "metadata": {},
   "source": [
    "A common computational pattern in many applications is the *reduction*.\n",
    "Examples are summing all elements of a vector, computing a dot product, or finding a minimum value in an array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66108f0-6d3c-4b28-b136-867de8de2b58",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## The Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c264629-9e92-49e6-8690-ea8971eaeb91",
   "metadata": {},
   "source": [
    "On GPUs, the main challenge in parallelizing reductions is the inherent race condition.\n",
    "Consider the following CPU function and its corresponding GPU kernel:\n",
    "\n",
    "```cpp\n",
    "void reduce(double *data, size_t nx) {\n",
    "    double sum = 0;\n",
    "\n",
    "    for (size_t i0 = 0; i0 < nx; ++i0)\n",
    "        sum += data[i0];\n",
    "\n",
    "    return sum;\n",
    "}\n",
    "```\n",
    "\n",
    "```cpp\n",
    "__global__ void reduce(double *data, double* sum, size_t nx) {\n",
    "    const size_t i0 = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (i0 < nx)\n",
    "        *sum += data[i0];\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198c8a9c",
   "metadata": {},
   "source": [
    "The compound assignment appears like a single operation, but actually involves several steps:\n",
    "* Loading the old value of `sum` from memory into a temporary variable (e.g., `tmp`)\n",
    "* Adding `data[i0]` to `tmp`\n",
    "* Writing the value of `tmp` back to `sum`\n",
    "\n",
    "When these steps are performed in parallel, some updates may be lost:\n",
    "* Multiple threads read `sum` concurrently\n",
    "* Each modifies its local copy\n",
    "* They write back their results, potentially overwriting each other's contributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8e05cf-e989-4b92-81fa-c499f1badd06",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## The Solution in CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d64b02-9f9b-4884-b777-45532c13f0ea",
   "metadata": {},
   "source": [
    "One solution is to ensure the compound assignment is performed as a single *atomic* operation:\n",
    "\n",
    "```cpp\n",
    "__global__ void reduce(double *data, double* sum, size_t nx) {\n",
    "    const size_t i0 = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (i0 < nx)\n",
    "        atomicAdd(sum, data[i0]);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8757db",
   "metadata": {},
   "source": [
    "While this version is correct, performance may suffer due to *atomic congestion*.\n",
    "Further optimization is possible through *hierarchical reduction*, which can include:\n",
    "* Each thread summing multiple input values\n",
    "* Each warp performing a reduction across its threads\n",
    "* Each block performing a reduction across its threads or warps\n",
    "\n",
    "A full discussion of all variants is beyond this tutorial, but a practical option is to use a block reduction with `cub`, a header-only library included in the Nvidia HPC Toolkit (or `hipCUB` on AMD):\n",
    "\n",
    "```cpp\n",
    "#include <cub/cub.cuh>\n",
    "\n",
    "template <unsigned int blockSize>\n",
    "__global__ void reduce(double *data, double* sum, size_t nx) {\n",
    "    const size_t i0 = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    // Define BlockReduce type for the block size\n",
    "    typedef cub::BlockReduce <double, blockSize, cub::BLOCK_REDUCE_RAKING_COMMUTATIVE_ONLY> BlockReduce;\n",
    "\n",
    "    // Allocate shared memory for block reduction\n",
    "    __shared__ typename BlockReduce::TempStorage tempStorage;\n",
    "\n",
    "    double elem = 0;\n",
    "\n",
    "    if (i0 < nx)\n",
    "        elem = data[i0];\n",
    "\n",
    "    // Reduce within the block (all threads *must* participate)\n",
    "    double blockSum = BlockReduce(tempStorage).Sum(elem);\n",
    "\n",
    "    // Atomically add the result to the global sum\n",
    "    if (0 == threadIdx.x && i0 < nx)\n",
    "        atomicAdd(sum, blockSum);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957be0c3",
   "metadata": {},
   "source": [
    "Other programming models also support reductions, with varying levels of programming effort, flexibility, and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48523d92",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## OpenMP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcec1a56",
   "metadata": {},
   "source": [
    "```cpp\n",
    "double sum = 0;\n",
    "\n",
    "#pragma omp target teams distribute parallel for \\\n",
    "            reduction(+ : sum)\n",
    "for (size_t i0 = 0; i0 < nx; ++i0)\n",
    "    sum += data[i0];\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5b246b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## OpenACC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abd1fb7",
   "metadata": {},
   "source": [
    "```cpp\n",
    "double sum = 0;\n",
    "\n",
    "#pragma acc parallel loop present(data[:nx]) \\\n",
    "            reduction(+ : sum)\n",
    "for (size_t i0 = 0; i0 < nx; ++i0)\n",
    "    sum += data[i0];\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81938072",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Modern C++"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7674c8d7",
   "metadata": {},
   "source": [
    "```cpp\n",
    "double sum = std::reduce(std::execution::par_unseq, data, data + nx, 0., std::plus<>{});\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b4d499",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Thrust"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af8f0a2",
   "metadata": {},
   "source": [
    "```cpp\n",
    "double sum = thrust::reduce(data, data + nx, 0.);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febf1b7b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Kokkos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b0ca95",
   "metadata": {},
   "source": [
    "```cpp\n",
    "double sum = 0;\n",
    "\n",
    "Kokkos::parallel_reduce(\n",
    "    Kokkos::RangePolicy<>(0, nx),\n",
    "    KOKKOS_LAMBDA(const size_t i0, double &acc) {\n",
    "        acc += data(i0);\n",
    "    }, sum);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a52550",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## SYCL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10e8edb",
   "metadata": {},
   "source": [
    "```cpp\n",
    "q.submit([&](sycl::handler &h) {\n",
    "    h.parallel_for(nx, [=](auto i0) {\n",
    "        auto v = sycl::atomic_ref<double, sycl::memory_order::relaxed,\n",
    "                                  sycl::memory_scope::device,\n",
    "                                  sycl::access::address_space::global_space>(\n",
    "            sum[0]);\n",
    "        v.fetch_add(data[i0]);\n",
    "    });\n",
    "});\n",
    "```\n",
    "\n",
    "Additional optimizations, similar to CUDA, are possible.\n",
    "For more information, see [Intel's OneAPI optimization guide](https://www.intel.com/content/www/us/en/docs/oneapi/optimization-guide-gpu/2025-0/reduction.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3c953c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Additional Consideration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84483102",
   "metadata": {},
   "source": [
    "Often, reductions can be fused with the production of their input values.\n",
    "For example, when computing a dot product, a naive implementation performs two steps:\n",
    "* Compute the point-wise multiplication and store the result in a temporary vector\n",
    "* Apply a sum reduction to the temporary vector\n",
    "\n",
    "Fusing these steps reduces memory usage and typically improves performance by minimizing data movement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a58e156",
   "metadata": {},
   "source": [
    "Most of the approaches discussed above support this fusion easily.\n",
    "Two exceptions are modern C++ and Thrust, which require different algorithms:\n",
    "* `std::transform_reduce`\n",
    "* `thrust::transform_reduce` or `thrust::transform_iterator`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
